# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I2S78zFRWeI7m0T7FF9jEKAFU0kcy1z8
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/MyDrive/6450FinalProj/Train_encoder

import json
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

class COCOTxtDataset(Dataset):
    def __init__(self, json_path):
        with open(json_path, "r") as f:
            data = json.load(f)
        # extract captions only
        self.captions = [ann["caption"] for ann in data["annotations"]]

    def __len__(self):
        return len(self.captions)

    def __getitem__(self, idx):
        return self.captions[idx]

train_set = COCOTxtDataset("annotations/captions_train2017.json")
val_set   = COCOTxtDataset("annotations/captions_val2017.json")

device = torch.device('cuda') if torch.cuda.is_available() else torch.device(
    'cpu')
device

from torch.utils.data import DataLoader

train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=4)
val_loader = DataLoader(val_set, batch_size=1, shuffle=False)

print(len(train_set))

from transformers import CLIPTokenizer, CLIPTextModel
from diffusers import StableDiffusionPipeline
stable = StableDiffusionPipeline.from_pretrained('runwayml/stable-diffusion-v1-5')

# teacher: SD1.5 CLIP
clip_sd = stable.text_encoder.to(device).eval()
print(clip_sd)

clip_new = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14").to(device).eval()
print(clip_new)

tokenizer = stable.tokenizer

import torch
import torch.nn as nn
import torch.nn.functional as F


class BigGtoCLIP_L14_Adapter(nn.Module):
    def __init__(self):
        super().__init__()

        self.proj = nn.Sequential(
            nn.Linear(1280, 1536),
            nn.GELU(),
            nn.Linear(1536, 768),
        )

        # Residual mixing gate (learns how much to trust BigG)
        self.gate = nn.Sequential(
            nn.Linear(1280, 256),
            nn.GELU(),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

        self.layernorm = nn.LayerNorm(768)

    def forward(self, bigg_emb):
        """
        bigg_emb : [B, 77, 1280] from ViT-bigG text encoder
        outputs  : [B, 77, 768]  aligned to ViT-L/14 space
        """
        proj = self.proj(bigg_emb)               # 1280 â†’ 768
        gate = self.gate(bigg_emb)               # [B,77,1]
        fused = proj * gate                      # gated enhancement
        return self.layernorm(fused)

def adapter_loss(pred, target, bigg_emb, lambda_mse=0.4,
                                  lambda_cos=0.3,
                                  lambda_dir=0.3):
    mse = F.mse_loss(pred, target)
    cos = 1 - F.cosine_similarity(pred, target, dim=-1).mean()
    idx = torch.randperm(pred.shape[0])
    pred_dir = F.normalize(pred - pred[idx], dim=-1)
    targ_dir = F.normalize(target - target[idx], dim=-1)
    dir_loss = 1 - (pred_dir * targ_dir).sum(dim=-1).mean()

    loss = (lambda_mse * mse +
            lambda_cos * cos +
            lambda_dir * dir_loss)

    return loss, {
        "mse": mse.item(),
        "cos": cos.item(),
        "dir": dir_loss.item()
    }

from transformers import CLIPTextModel, CLIPTokenizer

clip_teacher = CLIPTextModel.from_pretrained(
    "openai/clip-vit-large-patch14"
).to(device).eval()

clip_tokenizer = CLIPTokenizer.from_pretrained(
    "openai/clip-vit-large-patch14"
)

bigg_encoder = CLIPTextModel.from_pretrained(
    "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k"
).to(device).eval()

bigg_tokenizer = CLIPTokenizer.from_pretrained(
    "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k"
)

adapter = BigGtoCLIP_L14_Adapter().to(device)
optimizer = torch.optim.AdamW(adapter.parameters(), lr=1e-4)

sd = torch.load("clip_adapter.pth")
adapter.load_state_dict(sd)
batch_count = 0
for epoch in range(3):
    for texts in train_loader:
        # teacher (target)
        with torch.no_grad():
            tok = clip_tokenizer(texts, padding="max_length", max_length=77, return_tensors="pt").to(device)
            target = clip_teacher(tok.input_ids).last_hidden_state   # [B,77,768]

        # student (BigG)
        with torch.no_grad():
            tok_big = bigg_tokenizer(texts, padding="max_length", max_length=77, return_tensors="pt").to(device)
            bigg_emb = bigg_encoder(tok_big.input_ids).last_hidden_state  # [B,77,1280]

        # adapter
        pred = adapter(bigg_emb)

        loss, loss_dict = adapter_loss(pred, target, bigg_emb)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        batch_count += 1
        if batch_count % 1000 == 0:
            print(f"Batch {batch_count}, loss:", loss.item(), loss_dict)
            torch.save(adapter.state_dict(), "clip_adapter.pth")

import torch
torch.save(adapter.state_dict(), "clip_adapter.pth")

from transformers import T5EncoderModel
def direction_loss(t5, clip):
    idx = torch.randperm(t5.shape[0])
    t5_dir = F.normalize(t5_A - t5_B, dim=-1)
    clip_dir = F.normalize(clip_A - clip_B, dim=-1)
    return 1 - (t5_dir * clip_dir).mean()

class T5ProjectionTrainer(nn.Module):
    def __init__(self):
        super().__init__()

        self.projection = nn.Sequential(
            nn.Linear(512, 1024),
            nn.SiLU(),
            nn.Linear(1024, 768),
            nn.LayerNorm(768)
        )

    def forward(self, t5_emb, device="cuda"):
        return self.projection(t5_emb)


def train_t5_projection():
    t5_trainer = T5ProjectionTrainer().to(device)
    optimizer = torch.optim.AdamW(t5_trainer.projection.parameters(), lr=1e-4)

    stable = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
    clip_encoder = stable.text_encoder.to(device).eval()
    clip_tokenizer = stable.tokenizer
    t5_encoder = T5EncoderModel.from_pretrained("t5-small").to(device).eval()
    t5_tokenizer = T5Tokenizer.from_pretrained("t5-small", use_fast=False)
    batch_count = 0
    for epoch in range(3):
        for batch in train_loader:
            batch_count += 1
            texts = batch

            with torch.no_grad():
                clip_tokens = clip_tokenizer(
                    texts, padding="max_length", max_length=77, truncation=True, return_tensors="pt"
                ).to(device)
                clip_target = clip_encoder(clip_tokens.input_ids).last_hidden_state

            with torch.no_grad():
                t5_tokens = t5_tokenizer(
                    texts, return_tensors="pt", padding="max_length",
                    max_length=77, truncation=True
                ).to(device)
                t5_emb = t5_encoder(t5_tokens.input_ids).last_hidden_state
            t5_proj = t5_trainer(t5_emb)

            loss = direction_loss(t5_proj, clip_target)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            if loss <= 0.002:
                print(f"Early stopping on batch {batch_count}")
                break
            if batch_count % 1000 == 0:
                print(f"Batch {batch_count}, loss:", loss.item())

    torch.save(t5_trainer.projection.state_dict(), "t5_projection_pretrained.pth")

train_t5_projection()

from transformers import T5Tokenizer, T5EncoderModel
from diffusers import StableDiffusionPipeline

class MultiEncoderFusion(nn.Module):

    # fusion of a T5 encoder and a clip encoder, following similar structure of double encoder used in SDXL.
    def __init__(self):
        super().__init__()
        pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
        self.clip_encoder = pipe.text_encoder.eval()
        self.clip_tokenizer = pipe.tokenizer
        for param in self.clip_encoder.parameters():
            param.requires_grad = False # freeze encoders
        self.t5_encoder = T5EncoderModel.from_pretrained("t5-small").eval()
        for param in self.t5_encoder.parameters():
            param.requires_grad = False
        self.t5_tokenizer = T5Tokenizer.from_pretrained("t5-small", use_fast=False)

        self.t5_projection = nn.Sequential(
            nn.Linear(512, 1024),
            nn.SiLU(),
            nn.Linear(1024, 768),
            nn.LayerNorm(768)
        )
        self.attn = nn.MultiheadAttention(768, num_heads=8, batch_first=True, dropout=0.1)
        self.fuse_projection = nn.Sequential(
            nn.Linear(1536, 1536),
            nn.SiLU(),
            nn.Dropout(0.1),
            nn.Linear(1536, 768),
        )
        self.layer_norm = nn.LayerNorm(768)

    def forward(self, prompt, device="cuda"):
        with torch.no_grad():
            # CLIP
            clip_tokens = self.clip_tokenizer(
                prompt, padding="max_length", max_length=77, truncation=True, return_tensors="pt"
            ).to(device)
            clip_emb = self.clip_encoder(clip_tokens.input_ids).last_hidden_state # [b,77,768]

            # T5
            t5_tokens = self.t5_tokenizer(
                prompt, return_tensors="pt", padding="max_length", max_length=128, truncation=True
            ).to(device)
            t5_emb = self.t5_encoder(t5_tokens.input_ids).last_hidden_state # [b,L,512]


        t5_proj = self.t5_projection(t5_emb) # [b,L,768]
        t5_aligned, _ = self.attn(query=clip_emb, key=t5_proj, value=t5_proj) # [b, 77, 768]

        fused = torch.cat([clip_emb, t5_aligned], dim=-1)  # [b,77,1536]
        fused = self.layer_norm(self.fuse_projection(fused)+clip_emb) # Projection to fit SD1.5 input dim

        return fused

# Training fusion encoder
fusion_model = MultiEncoderFusion().to(device)
optimizer = torch.optim.AdamW(fusion_model.parameters(), lr=1e-4, weight_decay=0.01)

stable = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
clip_sd = stable.text_encoder.to(device).eval()
sd_tokenizer = stable.tokenizer

batch_count = 0
num_epoch = 5
for i in range(num_epoch):
    for batch in train_loader:
        texts = batch  # list[str]
        # clip_sd tokenizing and calculate embedding
        with torch.no_grad():
            tokens = sd_tokenizer(
                texts,
                padding="max_length",
                max_length=sd_tokenizer.model_max_length,
                truncation=True,
                return_tensors="pt",
            ).to(device)
            emb_sd = clip_sd(tokens.input_ids).last_hidden_state

        emb_aligned = fusion_model(texts)

        loss, loss_details = F.mse_loss(emb_aligned, emb_sd)


        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        batch_count += 1
        if loss <= 0.002:
            print(f"Early stopping on batch {batch_count}")
            break
        if batch_count % 1000 == 0:
            print(f"Batch {batch_count}, loss:", loss.item(), "details:", "mse:", loss_details["mse"], "cos:", loss_details["cos"], "dir:", loss_details["dir"])
            print("gate value:", fusion_model.gate_value)

torch.save(fusion_model.state_dict(), "fusion.pth")